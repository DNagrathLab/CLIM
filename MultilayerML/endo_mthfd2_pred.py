# -*- coding: utf-8 -*-
"""Endo_MTHFD2_pred.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kthA_dYdfpIQEsG5Ipc7ErKOaf_zaZMG

# Load Datasets
"""

# Export the TCGA and GENIE data
from google.colab import files
uploaded = files.upload()

# Load the data
import io
import pandas as pd
from google.colab import files

tcga_mutation = pd.read_table(io.BytesIO(uploaded['combined_TCGA_Endo_mutations.txt']))
tcga_status = pd.read_table(io.BytesIO(uploaded['combined_TCGA_Endo_OUTPUT19p.txt']))
tcga_mrna = pd.read_excel(io.BytesIO(uploaded['combined_TCGA_Endo_mRNA.xlsx']))

genie_mutation = pd.read_table(io.BytesIO(uploaded['GENIE_Endo_mutations.txt']))
genie_status = pd.read_table(io.BytesIO(uploaded['GENIE_Endo_Output19p.txt']))
genie_clinical = pd.read_table(io.BytesIO(uploaded['GENIE_Endo_Clinical.txt']))

tcga_summary = pd.read_excel(io.BytesIO(uploaded['combined_TCGA_Endo_OUTPUT_summaryResponseToMTHFD2.xlsx']))

"""# Pre-process the data"""

# Combine all TCGA datasets and divide it into train and test sets (80:20 split)
# This test set will be used to validate the second ML model
tcga_model2 = pd.merge(tcga_summary, tcga_mrna) # Combine TCGA mRNA and summary data
tcga_model2 = tcga_model2.dropna()
tcga_model2.index = tcga_model2['SAMPLE_ID']
tcga_model2 = tcga_model2.iloc[:, ~tcga_model2.columns.isin(['STUDY_ID', 'SAMPLE_ID', 'MTHFD2Thresh', 'UQCR11Thresh', 'MTHFD2', 'UQCR11' ])]

X = tcga_model2.iloc[ : , tcga_model2.columns != 'Summary' ]
y = tcga_model2.iloc[ : , tcga_model2.columns == 'Summary' ]

from sklearn.model_selection import train_test_split
X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size = 0.2, stratify = y,  random_state = 1)

# Combine the TCGA train dataset with the GENIE mutation data to get a merged mutation dataset
genie_mutation = genie_mutation.loc[  genie_mutation['SAMPLE_ID'].isin(genie_clinical['Sample ID'][genie_clinical['Sample Type'] == 'Primary']), :] # Only keep the GENIE data from primary samples
genie_mutation = genie_mutation.iloc[: , genie_mutation.columns != 'STUDY_ID' ]
genie_status = genie_status.rename(columns={'Gene Symbol': 'SAMPLE_ID' , 'LossUQCR11' :'Loss19p' })
genie_merged = pd.merge(genie_status, genie_mutation)
genie_merged.index = genie_merged['SAMPLE_ID']
genie_merged = genie_merged.iloc[:, genie_merged.columns != 'SAMPLE_ID']

tcga_model1 = pd.merge(tcga_summary, tcga_mutation)
tcga_model1.index = tcga_model1['SAMPLE_ID']
tcga_model1 = tcga_model1.iloc[ tcga_model1.index.isin(X_train2.index) , ~tcga_model1.columns.isin(['STUDY_ID', 'SAMPLE_ID', 'MTHFD2Thresh', 'UQCR11Thresh', 'Summary'])]

model1_merged = pd.concat([tcga_model1, genie_merged], axis=0)
X1 = model1_merged.iloc[ : , model1_merged.columns != 'Loss19p']
y1 = model1_merged.iloc[ : , model1_merged.columns == 'Loss19p']

# Divide the merged mutation dataset into train and test sets (80:20 split)
# This set will be used to validate the first ML model
X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size = 0.2, stratify = y1,  random_state = 1)

"""# Model 1: Decision Tree to predict 19p13 deletion from mutation information

## Feature Selection
"""

# Chi-squared test to eliminate mutations that are independent of 19p13 deletion
from sklearn.feature_selection import chi2
stat, pval = chi2(X_train1,y_train1)
independent_mutations = X_train1.loc[:,pval > 0.05/54].columns # Removed ['PIK3CA', 'CHD4', 'FBXW7', 'ARHGAP35', 'RB1', 'CHD2']
print(independent_mutations)
X_train1 = X_train1.iloc[:, ~X_train1.columns.isin(independent_mutations) ]

# Perform feature selection by RFE (recursive feature elimination)
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.model_selection import StratifiedKFold
from sklearn.feature_selection import RFECV

svc = SVC(kernel="linear")
rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(4),
              scoring='accuracy')
rfecv.fit(X_train1, y_train1.values.ravel())

print("Optimal number of features : %d" % rfecv.n_features_)

print(X_train1.columns[rfecv.ranking_ == 1] ) # 3 mutations ranked high: ['PTEN', 'TP53', 'POLE']

features = X_train1.columns[rfecv.ranking_ == 1]

# Subset the training and testing data to only keep the selected features
X_train = X_train1.iloc[:, X_train1.columns.isin(features) ] 
X_test = X_test1.iloc[:, X_test1.columns.isin(features) ] 
y_train = y_train1
y_test = y_test1

"""## Train Decision Tree Model"""

# Model training

from sklearn import tree
from sklearn.model_selection import GridSearchCV

tree = tree.DecisionTreeClassifier(random_state = 0)

# Five-fold cross-validation for hyperparameter optimization
params = {'criterion': ['gini', 'entropy'],
          'max_depth': list(range(2,15)),
          'min_samples_leaf' : list(range(1,4,1))
          }
model1 = GridSearchCV(tree, params, n_jobs = -1, cv = 5, scoring = 'f1')
model1.fit(X_train, y_train)
print(model1.best_params_)

y_pred = model1.predict(X_test)
y_pred_train = model1.predict(X_train)

# Calculate the performance metrics of the trained model
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

# Performance on the training data
print('\n')
print('Train accuracy : ' + str(accuracy_score(y_train, y_pred_train)))
print('Train Precision : ' + str(precision_score(y_train, y_pred_train,pos_label = 1)))
print('Train Recall : ' + str(recall_score(y_train, y_pred_train,pos_label = 1)))
print('Train F1 score : ' + str(f1_score(y_train, y_pred_train,pos_label = 1)))

# Performance of the test data
print('Test accuracy : ' + str(accuracy_score(y_test, y_pred)))
print('Test Precision : ' + str(precision_score(y_test, y_pred,pos_label = 1)))
print('Test Recall : ' + str(recall_score(y_test, y_pred,pos_label = 1)))
print('Test F1 score : ' + str(f1_score(y_test, y_pred,pos_label = 1)))

print('\n')

# Save the prediction results
predicted_values = pd.concat([pd.DataFrame(data = y_pred, columns = ['19p13_prediction'], index = y_test.index), pd.DataFrame(data = y_pred_train, columns = ['19p13_prediction'], index = y_train.index)])
predicted_values.to_csv('19p13Pred_endo_1.csv')
files.download('19p13Pred_endo_1.csv')

# Plot the tree to visualise the results
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
fig = plt.figure(frameon = False)
fig.set_size_inches(20, 12)
ax = plt.Axes(fig, [0., 0., 1., 1.], )
ax.set_axis_off()
fig.add_axes(ax)
plot_tree(model1.best_estimator_, filled=True)
print(X_train.columns)

"""## Test Other Models """

# Random Forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

forest = RandomForestClassifier(criterion='gini', n_estimators=50, random_state=0,n_jobs=2, bootstrap = True)

params = {'max_leaf_nodes': list(range(2,15))}
grid_search_cv = GridSearchCV(forest, params, n_jobs = -1, cv = 5, scoring = 'f1')
grid_search_cv.fit(X_train, y_train.values.ravel())
print(grid_search_cv.best_params_)

y_pred = grid_search_cv.predict(X_test)
y_pred_train = grid_search_cv.predict(X_train)

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

print('\n')
print('Train accuracy : ' + str(accuracy_score(y_train, y_pred_train)))
print('Train Precision : ' + str(precision_score(y_train, y_pred_train,pos_label = 1)))
print('Train Recall : ' + str(recall_score(y_train, y_pred_train,pos_label = 1)))
print('Train F1 score : ' + str(f1_score(y_train, y_pred_train,pos_label = 1)))

print('\n')
print('Test accuracy : ' + str(accuracy_score(y_test, y_pred)))
print('Test Precision : ' + str(precision_score(y_test, y_pred,pos_label = 1)))
print('Test Recall : ' + str(recall_score(y_test, y_pred,pos_label = 1)))
print('Test F1 score : ' + str(f1_score(y_test, y_pred,pos_label = 1)))

print('\n')

# Adaboost model

from sklearn.ensemble import AdaBoostClassifier

clf = AdaBoostClassifier(n_estimators=50, random_state=10)
params = {'n_estimators': list(range(20,40))}
grid_search_cv = GridSearchCV(clf, params, n_jobs = -1, cv = 5, scoring = 'f1')
grid_search_cv.fit(X_train, y_train.values.ravel())
print(grid_search_cv.best_params_)

y_pred = grid_search_cv.predict(X_test)
y_pred_train = grid_search_cv.predict(X_train)

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

print('\n')
print('Train accuracy : ' + str(accuracy_score(y_train, y_pred_train)))
print('Train Precision : ' + str(precision_score(y_train, y_pred_train,pos_label = 1)))
print('Train Recall : ' + str(recall_score(y_train, y_pred_train,pos_label = 1)))
print('Train F1 score : ' + str(f1_score(y_train, y_pred_train,pos_label = 1)))

print('\n')
print('Test accuracy : ' + str(accuracy_score(y_test, y_pred)))
print('Test Precision : ' + str(precision_score(y_test, y_pred,pos_label = 1)))
print('Test Recall : ' + str(recall_score(y_test, y_pred,pos_label = 1)))
print('Test F1 score : ' + str(f1_score(y_test, y_pred,pos_label = 1)))


print('\n')

# KNN model

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV

knn = KNeighborsClassifier()

params = {'n_neighbors':list(range(1,20)),
          'metric': ['minkowski','euclidean', 'manhattan', 'chebyshev' ],
          'p' : [1,1]}
grid_search_cv = GridSearchCV(knn, params, n_jobs = -1, cv = 5, scoring = 'f1')
grid_search_cv.fit(X_train, y_train.values.ravel())
print(grid_search_cv.best_params_)

y_pred = grid_search_cv.predict(X_test)
y_pred_train = grid_search_cv.predict(X_train)

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

print('\n')
print('Train accuracy : ' + str(accuracy_score(y_train, y_pred_train)))
print('Train Precision : ' + str(precision_score(y_train, y_pred_train,pos_label = 1)))
print('Train Recall : ' + str(recall_score(y_train, y_pred_train,pos_label = 1)))
print('Train F1 score : ' + str(f1_score(y_train, y_pred_train,pos_label = 1)))
print('\n')
print('Test accuracy : ' + str(accuracy_score(y_test, y_pred)))
print('Test Precision : ' + str(precision_score(y_test, y_pred,pos_label = 1)))
print('Test Recall : ' + str(recall_score(y_test, y_pred,pos_label = 1)))
print('Test F1 score : ' + str(f1_score(y_test, y_pred,pos_label = 1)))

print('\n')

# gaussian naive bayes model
#No hyperparameters to optimize

from sklearn.naive_bayes import GaussianNB

gnb = GaussianNB()
gnb.fit(X_train, y_train.values.ravel())

y_pred = gnb.predict(X_test)
y_pred_train = gnb.predict(X_train)

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

print('\n')
print('Train accuracy : ' + str(accuracy_score(y_train, y_pred_train)))
print('Train Precision : ' + str(precision_score(y_train, y_pred_train,pos_label = 1)))
print('Train Recall : ' + str(recall_score(y_train, y_pred_train,pos_label = 1)))
print('Train F1 score : ' + str(f1_score(y_train, y_pred_train,pos_label = 1)))
print('\n')

print('Test accuracy : ' + str(accuracy_score(y_test, y_pred)))
print('Test Precision : ' + str(precision_score(y_test, y_pred,pos_label = 1)))
print('Test Recall : ' + str(recall_score(y_test, y_pred,pos_label = 1)))
print('Test F1 score : ' + str(f1_score(y_test, y_pred,pos_label = 1)))

print('\n')

# Linear SVM 

from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV

from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV

lsvm = SVC(kernel='linear', random_state=0)

params = {'C': [0.02, 0.03, 0.05, 0.075, 0.1, 0.2, 0.35, 0.5, 0.7, 0.9] } 
grid_search_cv = GridSearchCV(lsvm, params, n_jobs = -1, cv = 5, scoring = 'f1')
grid_search_cv.fit(X_train, y_train.values.ravel())
print(grid_search_cv.best_params_)

y_pred = grid_search_cv.predict(X_test)
y_pred_train = grid_search_cv.predict(X_train)

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

print('\n')
print('Train accuracy : ' + str(accuracy_score(y_train, y_pred_train)))
print('Train Precision : ' + str(precision_score(y_train, y_pred_train,pos_label = 1)))
print('Train Recall : ' + str(recall_score(y_train, y_pred_train,pos_label = 1)))
print('Train F1 score : ' + str(f1_score(y_train, y_pred_train,pos_label = 1)))
print('\n')
print('Test accuracy : ' + str(accuracy_score(y_test, y_pred)))
print('Test Precision : ' + str(precision_score(y_test, y_pred,pos_label = 1)))
print('Test Recall : ' + str(recall_score(y_test, y_pred,pos_label = 1)))
print('Test F1 score : ' + str(f1_score(y_test, y_pred,pos_label = 1)))

print('\n')

# LDA model
# No hyperparameters to optimize

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

lda = LDA()
lda.fit(X_train, y_train.values.ravel())

y_pred = lda.predict(X_test)
y_pred_train = lda.predict(X_train)

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

print('\n')
print('Train accuracy : ' + str(accuracy_score(y_train, y_pred_train)))
print('Train Precision : ' + str(precision_score(y_train, y_pred_train,pos_label = 1)))
print('Train Recall : ' + str(recall_score(y_train, y_pred_train,pos_label = 1)))
print('Train F1 score : ' + str(f1_score(y_train, y_pred_train,pos_label = 1)))
print('\n')
print('Test accuracy : ' + str(accuracy_score(y_test, y_pred)))
print('Test Precision : ' + str(precision_score(y_test, y_pred,pos_label = 1)))
print('Test Recall : ' + str(recall_score(y_test, y_pred,pos_label = 1)))
print('Test F1 score : ' + str(f1_score(y_test, y_pred,pos_label = 1)))

print('\n')

# Logistic regression model

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

lr = LogisticRegression(random_state=0, solver='liblinear')

params = {'C': [0.001, 0.0025, 0.005, 0.0075, 0.01, 0.015, 0.02, 0.03, 0.05, 0.075, 0.1, 0.2, 0.35, 0.5, 0.7, 0.9]  } # C = 0.01
grid_search_cv = GridSearchCV(lr, params, n_jobs = -1, cv = 5,  scoring = 'f1')
grid_search_cv.fit(X_train, y_train.values.ravel())
print(grid_search_cv.best_params_)

y_pred = grid_search_cv.predict(X_test)
y_pred_train = grid_search_cv.predict(X_train)

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

print('\n')
print('Train accuracy : ' + str(accuracy_score(y_train, y_pred_train)))
print('Train Precision : ' + str(precision_score(y_train, y_pred_train,pos_label = 1)))
print('Train Recall : ' + str(recall_score(y_train, y_pred_train,pos_label = 1)))
print('Train F1 score : ' + str(f1_score(y_train, y_pred_train,pos_label = 1)))
print('\n')
print('Test accuracy : ' + str(accuracy_score(y_test, y_pred)))
print('Test Precision : ' + str(precision_score(y_test, y_pred,pos_label = 1)))
print('Test Recall : ' + str(recall_score(y_test, y_pred,pos_label = 1)))
print('Test F1 score : ' + str(f1_score(y_test, y_pred,pos_label = 1)))


print('\n')

# rfb SVM model

from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV

rsvm = SVC(kernel='rbf', random_state=0)

params = {'C': [0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 0.1 ],  # C =0.075
   'gamma': [0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 0.1 ] } # gamma = 0.1

grid_search_cv = GridSearchCV(rsvm, params, n_jobs = -1, cv = 5, scoring = 'f1')
grid_search_cv.fit(X_train, y_train.values.ravel())
print(grid_search_cv.best_params_)

y_pred = grid_search_cv.predict(X_test)
y_pred_train = grid_search_cv.predict(X_train)

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

print('\n')
print('Train accuracy : ' + str(accuracy_score(y_train, y_pred_train)))
print('Train Precision : ' + str(precision_score(y_train, y_pred_train,pos_label = 1)))
print('Train Recall : ' + str(recall_score(y_train, y_pred_train,pos_label = 1)))
print('Train F1 score : ' + str(f1_score(y_train, y_pred_train,pos_label = 1)))

print('\n')
print('Test accuracy : ' + str(accuracy_score(y_test, y_pred)))
print('Test Precision : ' + str(precision_score(y_test, y_pred,pos_label = 1)))
print('Test Recall : ' + str(recall_score(y_test, y_pred,pos_label = 1)))
print('Test F1 score : ' + str(f1_score(y_test, y_pred,pos_label = 1)))

print('\n')

#  XGBoost model
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV

xgb = XGBClassifier(booster = 'gbtree', random_state=0)

params = {
        'min_child_weight': [1, 5, 10],
        'gamma': [0.5, 1, 1.5, 2, 5, 10],
        'subsample': [0.6, 0.8, 1.0],
        'colsample_bytree': [0.6, 0.8, 1.0],
        'max_depth': list(range(2,8))
        }

grid_search_cv = GridSearchCV(xgb, params, n_jobs = -1, cv = 5, scoring = 'f1')
grid_search_cv.fit(X_train, y_train.values.ravel())
print(grid_search_cv.best_params_)

y_pred = grid_search_cv.predict(X_test)
y_pred_train = grid_search_cv.predict(X_train)

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

print('\n')
print('Train accuracy : ' + str(accuracy_score(y_train, y_pred_train)))
print('Train Precision : ' + str(precision_score(y_train, y_pred_train,pos_label = 1)))
print('Train Recall : ' + str(recall_score(y_train, y_pred_train,pos_label = 1)))
print('Train F1 score : ' + str(f1_score(y_train, y_pred_train,pos_label = 1)))

print('\n')

print('Test accuracy : ' + str(accuracy_score(y_test, y_pred)))
print('Test Precision : ' + str(precision_score(y_test, y_pred,pos_label = 1)))
print('Test Recall : ' + str(recall_score(y_test, y_pred,pos_label = 1)))
print('Test F1 score : ' + str(f1_score(y_test, y_pred,pos_label = 1)))
print('Test F1 score (0) : ' + str(f1_score(y_test, y_pred,pos_label = 0)))

print('\n')

"""## Plot the PR curve

### PR curve for test data
"""

# The previously optimized hyperparameters were used to create the plots

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_recall_curve
from matplotlib import pyplot

# Gaussuan NB model
from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
gnb.fit(X_train, y_train.values.ravel())
gnb_probs = gnb.predict_proba(X_test)[: , 1] # predict probabilities
gnb_precision, gnb_recall, _ = precision_recall_curve(y_test, gnb_probs)


## AdaBoost model
from sklearn.ensemble import AdaBoostClassifier
clf = AdaBoostClassifier(random_state=10,n_estimators = 20)
clf.fit(X_train, y_train.values.ravel())
clf_probs = clf.predict_proba(X_test)[: , 1] # predict probabilities
clf_precision, clf_recall, _ = precision_recall_curve(y_test, clf_probs)


## Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(criterion='gini',max_leaf_nodes = 2, n_estimators=50, random_state=0,n_jobs=2, bootstrap = True)
forest.fit(X_train, y_train.values.ravel())
forest_probs = forest.predict_proba(X_test)[: , 1] # predict probabilities
forest_precision, forest_recall, _ = precision_recall_curve(y_test, forest_probs)


## Linear SVM
from sklearn.svm import SVC
lsvm = SVC(kernel='linear', C = 0.03, random_state=0, probability=True)
lsvm.fit(X_train, y_train.values.ravel())
lsvm_probs = lsvm.predict_proba(X_test)[: , 1] # predict probabilities
lsvm_precision, lsvm_recall, _ = precision_recall_curve(y_test, lsvm_probs)

## KNN
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors = 15, metric = 'minkowski', p = 1)
knn.fit(X_train, y_train.values.ravel())
knn_probs = knn.predict_proba(X_test)[: , 1] # predict probabilities
knn_precision, knn_recall, _ = precision_recall_curve(y_test, knn_probs)


## Logistic Regression
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(random_state=0, solver='liblinear', C = 0.01)
lr.fit(X_train, y_train.values.ravel())
lr_probs = lr.predict_proba(X_test)[: , 1] # predict probabilities
lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs)

## Decision Tree
from sklearn import tree
tree = tree.DecisionTreeClassifier(random_state = 0, criterion = 'gini', max_depth = 2,min_samples_leaf = 1)
tree.fit(X_train, y_train.values.ravel())
tree_probs = tree.predict_proba(X_test)[: , 1] # predict probabilities
tree_precision, tree_recall, _ = precision_recall_curve(y_test, tree_probs)

## RBF SVM
from sklearn.svm import SVC
rsvm = SVC(kernel='rbf', random_state=0, C = 0.025, gamma = 0.5, probability = True)
rsvm.fit(X_train, y_train.values.ravel())
rsvm_probs = rsvm.predict_proba(X_test)[: , 1] # predict probabilities
rsvm_precision, rsvm_recall, _ = precision_recall_curve(y_test, rsvm_probs)

## LDA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
lda = LDA()
lda.fit(X_train, y_train.values.ravel())
lda_probs = lda.predict_proba(X_test)[: , 1] # predict probabilities
lda_precision, lda_recall, _ = precision_recall_curve(y_test, lda_probs)

## XGBoost model
from xgboost import XGBClassifier
xgb = XGBClassifier(booster = 'gbtree', random_state=0, min_child_weight = 5, gamma = 1.5, subsample = 0.8,  colsample_bytree = 0.8, max_depth = 2 )
xgb.fit(X_train, y_train.values.ravel())
xgb_probs = xgb.predict_proba(X_test)[: , 1] # predict probabilities
xgb_precision, xgb_recall, _ = precision_recall_curve(y_test, xgb_probs)

# plot the precision-recall curves
from matplotlib.pyplot import figure
figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')
pyplot.plot(gnb_recall, gnb_precision, marker='.', label='Gaussian NB')
pyplot.plot(clf_recall, clf_precision, marker='.', label='AdaBoost')
pyplot.plot(forest_recall, forest_precision, marker='.', label='Random Forest')
pyplot.plot(lsvm_recall, lsvm_precision, marker='.', label='Linear SVM')
pyplot.plot(lda_recall, lda_precision, marker='.', label='LDA')
pyplot.plot(rsvm_recall, rsvm_precision, marker='.', label='RBF SVM')
pyplot.plot(knn_recall, knn_precision, marker='.', label='KNN')
pyplot.plot(tree_recall, tree_precision, marker='.', label='Decision Tree')
pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic Regression')
pyplot.plot(xgb_recall, xgb_precision, marker='.', label='XGBoost')
pyplot.xlabel('Recall')
pyplot.ylabel('Precision')
pyplot.legend(loc = 'upper right', bbox_to_anchor=(1.4, 0.7)) # show the legend
pyplot.show() # show the plot

# Save the PR data to create plots offline

import numpy as np
nan = np.empty(250)
nan[:] = np.nan

PR_all = pd.DataFrame(data = { 'GaussianNB Precision': np.append(gnb_precision, nan[0:250-len(gnb_precision)]) , 'GaussianNB Recall': np.append(gnb_recall, nan[0:250-len(gnb_precision)]) , 
                              'Random Forest Precision': np.append(forest_precision, nan[0:250-len(forest_precision)]) , 'Random Forest Recall': np.append(forest_recall, nan[0:250-len(forest_precision)]) , 
                              'AdaBoost Precision': np.append(clf_precision, nan[0:250-len(clf_precision)]) , 'AdaBoost Recall': np.append(clf_recall, nan[0:250-len(clf_precision)]),
                              'KNN Precision': np.append(knn_precision, nan[0:250-len(knn_precision)]) , 'LNN Recall': np.append(knn_recall, nan[0:250-len(knn_precision)]),
                              'LDA Precision': np.append(lda_precision, nan[0:250-len(lda_precision)]) , 'LDA Recall': np.append(lda_recall, nan[0:250-len(lda_precision)]),
                              'Decision Tree Precision': np.append(tree_precision, nan[0:250-len(tree_precision)]) , 'Decision Tree Recall': np.append(tree_recall, nan[0:250-len(tree_precision)]),
                              'Logistic Regression Precision': np.append(lr_precision, nan[0:250-len(lr_precision)]) , 'Logistic Regression Recall': np.append(lr_recall, nan[0:250-len(lr_precision)]),
                              'RBF SVM Precision': np.append(rsvm_precision, nan[0:250-len(rsvm_precision)]) , 'RBF SVM Recall': np.append(rsvm_recall, nan[0:250-len(rsvm_precision)]),
                              'Linear SVM Precision': np.append(lsvm_precision, nan[0:250-len(lsvm_precision)]) , 'Linear SVM Recall': np.append(lsvm_recall, nan[0:250-len(lsvm_precision)]),
                              'XGBoost Precision': np.append(xgb_precision, nan[0:250-len(xgb_precision)]) , 'XGBoost Recall': np.append(xgb_recall, nan[0:250-len(xgb_precision)])                                
                              })
PR_all.to_csv('PR_test_model1_endo.csv')
files.download('PR_test_model1_endo.csv')

"""### PR curve for all data (test+train)"""

# The previously optimized hyperparameters were used to create the plots

X_all = pd.concat([X_train, X_test])
y_all = pd.concat([y_train, y_test])

X_test = X_all
y_test = y_all

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_recall_curve
from matplotlib import pyplot

# Gaussuan NB model
from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
gnb.fit(X_train, y_train.values.ravel())
gnb_probs = gnb.predict_proba(X_test)[: , 1] # predict probabilities
gnb_precision, gnb_recall, _ = precision_recall_curve(y_test, gnb_probs)


## AdaBoost model
from sklearn.ensemble import AdaBoostClassifier
clf = AdaBoostClassifier(random_state=10,n_estimators = 20)
clf.fit(X_train, y_train.values.ravel())
clf_probs = clf.predict_proba(X_test)[: , 1] # predict probabilities
clf_precision, clf_recall, _ = precision_recall_curve(y_test, clf_probs)


## Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(criterion='gini',max_leaf_nodes = 2, n_estimators=50, random_state=0,n_jobs=2, bootstrap = True)
forest.fit(X_train, y_train.values.ravel())
forest_probs = forest.predict_proba(X_test)[: , 1] # predict probabilities
forest_precision, forest_recall, _ = precision_recall_curve(y_test, forest_probs)


## Linear SVM
from sklearn.svm import SVC
lsvm = SVC(kernel='linear', C = 0.03, random_state=0, probability=True)
lsvm.fit(X_train, y_train.values.ravel())
lsvm_probs = lsvm.predict_proba(X_test)[: , 1] # predict probabilities
lsvm_precision, lsvm_recall, _ = precision_recall_curve(y_test, lsvm_probs)

## KNN
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors = 15, metric = 'minkowski', p = 1)
knn.fit(X_train, y_train.values.ravel())
knn_probs = knn.predict_proba(X_test)[: , 1] # predict probabilities
knn_precision, knn_recall, _ = precision_recall_curve(y_test, knn_probs)


## Logistic Regression
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(random_state=0, solver='liblinear', C = 0.01)
lr.fit(X_train, y_train.values.ravel())
lr_probs = lr.predict_proba(X_test)[: , 1] # predict probabilities
lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs)

## Decision Tree
from sklearn import tree
tree = tree.DecisionTreeClassifier(random_state = 0, criterion = 'gini', max_depth = 2,min_samples_leaf = 1)
tree.fit(X_train, y_train.values.ravel())
tree_probs = tree.predict_proba(X_test)[: , 1] # predict probabilities
tree_precision, tree_recall, _ = precision_recall_curve(y_test, tree_probs)

## RBF SVM
from sklearn.svm import SVC
rsvm = SVC(kernel='rbf', random_state=0, C = 0.025, gamma = 0.5, probability = True)
rsvm.fit(X_train, y_train.values.ravel())
rsvm_probs = rsvm.predict_proba(X_test)[: , 1] # predict probabilities
rsvm_precision, rsvm_recall, _ = precision_recall_curve(y_test, rsvm_probs)

## LDA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
lda = LDA()
lda.fit(X_train, y_train.values.ravel())
lda_probs = lda.predict_proba(X_test)[: , 1] # predict probabilities
lda_precision, lda_recall, _ = precision_recall_curve(y_test, lda_probs)

## XGBoost model
from xgboost import XGBClassifier
xgb = XGBClassifier(booster = 'gbtree', random_state=0, min_child_weight = 5, gamma = 1.5, subsample = 0.8,  colsample_bytree = 0.8, max_depth = 2 )
xgb.fit(X_train, y_train.values.ravel())
xgb_probs = xgb.predict_proba(X_test)[: , 1] # predict probabilities
xgb_precision, xgb_recall, _ = precision_recall_curve(y_test, xgb_probs)

# plot the precision-recall curves
from matplotlib.pyplot import figure
figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')
pyplot.plot(gnb_recall, gnb_precision, marker='.', label='Gaussian NB')
pyplot.plot(clf_recall, clf_precision, marker='.', label='AdaBoost')
pyplot.plot(forest_recall, forest_precision, marker='.', label='Random Forest')
pyplot.plot(lsvm_recall, lsvm_precision, marker='.', label='Linear SVM')
pyplot.plot(lda_recall, lda_precision, marker='.', label='LDA')
pyplot.plot(rsvm_recall, rsvm_precision, marker='.', label='RBF SVM')
pyplot.plot(knn_recall, knn_precision, marker='.', label='KNN')
pyplot.plot(tree_recall, tree_precision, marker='.', label='Decision Tree')
pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic Regression')
pyplot.plot(xgb_recall, xgb_precision, marker='.', label='XGBoost')
pyplot.xlabel('Recall')
pyplot.ylabel('Precision')
pyplot.legend(loc = 'upper right', bbox_to_anchor=(1.4, 0.7)) # show the legend
pyplot.show() # show the plot

# Save the PR data to create plots offline

import numpy as np
nan = np.empty(250)
nan[:] = np.nan

PR_all = pd.DataFrame(data = { 'GaussianNB Precision': np.append(gnb_precision, nan[0:250-len(gnb_precision)]) , 'GaussianNB Recall': np.append(gnb_recall, nan[0:250-len(gnb_precision)]) , 
                              'Random Forest Precision': np.append(forest_precision, nan[0:250-len(forest_precision)]) , 'Random Forest Recall': np.append(forest_recall, nan[0:250-len(forest_precision)]) , 
                              'AdaBoost Precision': np.append(clf_precision, nan[0:250-len(clf_precision)]) , 'AdaBoost Recall': np.append(clf_recall, nan[0:250-len(clf_precision)]),
                              'KNN Precision': np.append(knn_precision, nan[0:250-len(knn_precision)]) , 'LNN Recall': np.append(knn_recall, nan[0:250-len(knn_precision)]),
                              'LDA Precision': np.append(lda_precision, nan[0:250-len(lda_precision)]) , 'LDA Recall': np.append(lda_recall, nan[0:250-len(lda_precision)]),
                              'Decision Tree Precision': np.append(tree_precision, nan[0:250-len(tree_precision)]) , 'Decision Tree Recall': np.append(tree_recall, nan[0:250-len(tree_precision)]),
                              'Logistic Regression Precision': np.append(lr_precision, nan[0:250-len(lr_precision)]) , 'Logistic Regression Recall': np.append(lr_recall, nan[0:250-len(lr_precision)]),
                              'RBF SVM Precision': np.append(rsvm_precision, nan[0:250-len(rsvm_precision)]) , 'RBF SVM Recall': np.append(rsvm_recall, nan[0:250-len(rsvm_precision)]),
                              'Linear SVM Precision': np.append(lsvm_precision, nan[0:250-len(lsvm_precision)]) , 'Linear SVM Recall': np.append(lsvm_recall, nan[0:250-len(lsvm_precision)]),
                              'XGBoost Precision': np.append(xgb_precision, nan[0:250-len(xgb_precision)]) , 'XGBoost Recall': np.append(xgb_recall, nan[0:250-len(xgb_precision)])                                
                              })
PR_all.to_csv('PR_all_model1_endo.csv')
files.download('PR_all_model1_endo.csv')

"""# Model 2: Model 1 prediction + mitochondrial gene expression to predict therapy respose in TCGA samples

## Pre-processing and feature selection
"""

# Add the 19p13DEL prediction to the training data
model1_train = pd.merge(tcga_summary, tcga_mutation)
model1_train = model1_train.dropna()
model1_train.index = model1_train['SAMPLE_ID']
model1_train = model1_train.iloc[:, model1_train.columns.isin(features)] 
model1_output = pd.DataFrame(data = model1.predict(model1_train), index = model1_train.index, columns= ['19p13Pred'])

# Combine the output with the test and train sets
X_train2 = pd.merge(X_train2, model1_output.iloc[ model1_output.index.isin(X_train2.index) ,:], left_index=True, right_index=True)
X_test2 = pd.merge(X_test2, model1_output.iloc[ model1_output.index.isin(X_test2.index) ,:], left_index=True, right_index=True)

# Remove the previously used features from X_train2 and X_test2
X_train2 = X_train2.iloc[:, ~X_train2.columns.isin(['Loss19p'])]
X_test2 = X_test2.iloc[:, ~X_test2.columns.isin(['Loss19p'])]

# Perfrorm feature selection by RFE

import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.model_selection import StratifiedKFold
from sklearn.feature_selection import RFECV

svc = SVC(kernel="linear")
rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(4),
              scoring='f1')
rfecv.fit(X_train2, y_train2.values.ravel())

print("Optimal number of features : %d" % rfecv.n_features_)
print(X_train2.columns[rfecv.ranking_ == 1] )
features_optimum = X_train2.columns[rfecv.ranking_ == 1]

# Create the final test and train set
model2_Xtrain = X_train2.iloc[:, X_train2.columns.isin(features_optimum) ] 
model2_Xtest = X_test2.iloc[:, X_test2.columns.isin(features_optimum) ] 
model2_ytrain = y_train2
model2_ytest = y_test2

"""## Train different models to select the best one"""

# Train a decision tree model

from sklearn import tree
from sklearn.model_selection import GridSearchCV

tree = tree.DecisionTreeClassifier(random_state = 0)

params = {'criterion': ['gini', 'entropy'],
          'max_depth': list(range(2,15)),
          'min_samples_leaf' : list(range(1,4,1))
          }
grid_search_cv = GridSearchCV(tree, params, n_jobs = -1, cv = 5, scoring = 'f1')
grid_search_cv.fit(model2_Xtrain, model2_ytrain.values.ravel())
print(grid_search_cv.best_params_)

y_pred = grid_search_cv.predict(model2_Xtest)
y_pred_train = grid_search_cv.predict(model2_Xtrain)

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

print('\n')
print('Train accuracy : ' + str(accuracy_score(model2_ytrain, y_pred_train)))
print('Train Precision : ' + str(precision_score(model2_ytrain, y_pred_train,pos_label = 1)))
print('Train Recall : ' + str(recall_score(model2_ytrain, y_pred_train,pos_label = 1)))
print('Train F1 score : ' + str(f1_score(model2_ytrain, y_pred_train,pos_label = 1)))

print('\n')

print('Test accuracy : ' + str(accuracy_score(model2_ytest, y_pred)))
print('Test Precision : ' + str(precision_score(model2_ytest, y_pred,pos_label = 1)))
print('Test Recall : ' + str(recall_score(model2_ytest, y_pred,pos_label = 1)))
print('Test F1 score : ' + str(f1_score(model2_ytest, y_pred,pos_label = 1)))


print('\n')

###### Random forest classifier.  Perfrom a grid search to get the optimum hymerparameters

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

forest = RandomForestClassifier(criterion='gini', n_estimators=50, random_state=0,n_jobs=2, bootstrap = True)

params = {'max_leaf_nodes': list(range(2,15))}
grid_search_cv = GridSearchCV(forest, params, n_jobs = -1, cv = 5, scoring = 'f1')
grid_search_cv.fit(model2_Xtrain, model2_ytrain.values.ravel())
print(grid_search_cv.best_params_)

y_pred = grid_search_cv.predict(model2_Xtest)
y_pred_train = grid_search_cv.predict(model2_Xtrain)

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

print('\n')
print('Train accuracy : ' + str(accuracy_score(model2_ytrain, y_pred_train)))
print('Train Precision : ' + str(precision_score(model2_ytrain, y_pred_train,pos_label = 1)))
print('Train Recall : ' + str(recall_score(model2_ytrain, y_pred_train,pos_label = 1)))
print('Train F1 score : ' + str(f1_score(model2_ytrain, y_pred_train,pos_label = 1)))

print('\n')

print('Test accuracy : ' + str(accuracy_score(model2_ytest, y_pred)))
print('Test Precision : ' + str(precision_score(model2_ytest, y_pred,pos_label = 1)))
print('Test Recall : ' + str(recall_score(model2_ytest, y_pred,pos_label = 1)))
print('Test F1 score : ' + str(f1_score(model2_ytest, y_pred,pos_label = 1)))


print('\n')

########## Fit and test a kNN model to the data. Perfrom a grid search to get the optimum hymerparameters

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV

knn = KNeighborsClassifier()

params = {'n_neighbors':list(range(1,20)),
          'metric': ['minkowski','euclidean', 'manhattan', 'chebyshev' ],
          'p' : [1,1]}
grid_search_cv = GridSearchCV(knn, params, n_jobs = -1, cv = 5, scoring = 'f1')
grid_search_cv.fit(model2_Xtrain, model2_ytrain.values.ravel())
print(grid_search_cv.best_params_)

y_pred = grid_search_cv.predict(model2_Xtest)
y_pred_train = grid_search_cv.predict(model2_Xtrain)

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

print('\n')
print('Train accuracy : ' + str(accuracy_score(model2_ytrain, y_pred_train)))
print('Train Precision : ' + str(precision_score(model2_ytrain, y_pred_train,pos_label = 1)))
print('Train Recall : ' + str(recall_score(model2_ytrain, y_pred_train,pos_label = 1)))
print('Train F1 score : ' + str(f1_score(model2_ytrain, y_pred_train,pos_label = 1)))

print('\n')

print('Test accuracy : ' + str(accuracy_score(model2_ytest, y_pred)))
print('Test Precision : ' + str(precision_score(model2_ytest, y_pred,pos_label = 1)))
print('Test Recall : ' + str(recall_score(model2_ytest, y_pred,pos_label = 1)))
print('Test F1 score : ' + str(f1_score(model2_ytest, y_pred,pos_label = 1)))

print('\n')

########## Fit and test a logistic regression model to the data. Perfrom a grid search to get the optimum hymerparameters

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

lr = LogisticRegression(random_state=0, solver='liblinear')

params = {'C': [  0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9, 1, 1.1,1.2, 1.3, 1.4, 1.6, 1.7, 1.8, 1.9, 2, 2.5, 3, 3.5, 4.0, 4.5, 5, 6, 7, 8, 9, 10] } 
grid_search_cv = GridSearchCV(lr, params, n_jobs = -1, cv = 5,  scoring = 'f1')
grid_search_cv.fit(model2_Xtrain, model2_ytrain.values.ravel())
print(grid_search_cv.best_params_)

y_pred = grid_search_cv.predict(model2_Xtest)
y_pred_train = grid_search_cv.predict(model2_Xtrain)

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

print('\n')
print('Train accuracy : ' + str(accuracy_score(model2_ytrain, y_pred_train)))
print('Train Precision : ' + str(precision_score(model2_ytrain, y_pred_train,pos_label = 1)))
print('Train Recall : ' + str(recall_score(model2_ytrain, y_pred_train,pos_label = 1)))
print('Train F1 score : ' + str(f1_score(model2_ytrain, y_pred_train,pos_label = 1)))

print('\n')

print('Test accuracy : ' + str(accuracy_score(model2_ytest, y_pred)))
print('Test Precision : ' + str(precision_score(model2_ytest, y_pred,pos_label = 1)))
print('Test Recall : ' + str(recall_score(model2_ytest, y_pred,pos_label = 1)))
print('Test F1 score : ' + str(f1_score(model2_ytest, y_pred,pos_label = 1)))

print('\n')

########## Fit and test a gaussian naive bayes model to the data. No hyperparameters to optimize

from sklearn.naive_bayes import GaussianNB

gnb = GaussianNB()
gnb.fit(model2_Xtrain, model2_ytrain.values.ravel())

y_pred = gnb.predict(model2_Xtest)
y_pred_train = gnb.predict(model2_Xtrain)

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

print('\n')
print('Train accuracy : ' + str(accuracy_score(model2_ytrain, y_pred_train)))
print('Train Precision : ' + str(precision_score(model2_ytrain, y_pred_train,pos_label = 1)))
print('Train Recall : ' + str(recall_score(model2_ytrain, y_pred_train,pos_label = 1)))
print('Train F1 score : ' + str(f1_score(model2_ytrain, y_pred_train,pos_label = 1)))

print('\n')

print('Test accuracy : ' + str(accuracy_score(model2_ytest, y_pred)))
print('Test Precision : ' + str(precision_score(model2_ytest, y_pred,pos_label = 1)))
print('Test Recall : ' + str(recall_score(model2_ytest, y_pred,pos_label = 1)))
print('Test F1 score : ' + str(f1_score(model2_ytest, y_pred,pos_label = 1)))

print('\n')

########## Fit and test an LDA model to the data. No hyperparameters to optimize

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

lda = LDA()
lda.fit(model2_Xtrain, model2_ytrain.values.ravel())

y_pred = lda.predict(model2_Xtest)
y_pred_train = lda.predict(model2_Xtrain)

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

print('\n')
print('Train accuracy : ' + str(accuracy_score(model2_ytrain, y_pred_train)))
print('Train Precision : ' + str(precision_score(model2_ytrain, y_pred_train,pos_label = 1)))
print('Train Recall : ' + str(recall_score(model2_ytrain, y_pred_train,pos_label = 1)))
print('Train F1 score : ' + str(f1_score(model2_ytrain, y_pred_train,pos_label = 1)))
print('\nMetrics for class 0:')
print('Train Precision : ' + str(precision_score(model2_ytrain, y_pred_train,pos_label = 0)))
print('Train Recall : ' + str(recall_score(model2_ytrain, y_pred_train,pos_label = 0)))
print('Train F1 score : ' + str(f1_score(model2_ytrain, y_pred_train,pos_label = 0)))
print('\n')

print('Test accuracy : ' + str(accuracy_score(model2_ytest, y_pred)))
print('Test Precision : ' + str(precision_score(model2_ytest, y_pred,pos_label = 1)))
print('Test Recall : ' + str(recall_score(model2_ytest, y_pred,pos_label = 1)))
print('Test F1 score : ' + str(f1_score(model2_ytest, y_pred,pos_label = 1)))

print('\n')

########## Fit and test linear SVM to the data. Perfrom a grid search to get the optimum hymerparameters

from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV

from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV

lsvm = SVC(kernel='linear', random_state=0)


params = {'C': list(range(10, 200, 5))} 
grid_search_cv = GridSearchCV(lsvm, params, n_jobs = -1, cv = 5, scoring = 'f1')
grid_search_cv.fit(model2_Xtrain, model2_ytrain.values.ravel())
print(grid_search_cv.best_params_)

y_pred = grid_search_cv.predict(model2_Xtest)
y_pred_train = grid_search_cv.predict(model2_Xtrain)

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

print('\n')
print('Train accuracy : ' + str(accuracy_score(model2_ytrain, y_pred_train)))
print('Train Precision : ' + str(precision_score(model2_ytrain, y_pred_train,pos_label = 1)))
print('Train Recall : ' + str(recall_score(model2_ytrain, y_pred_train,pos_label = 1)))
print('Train F1 score : ' + str(f1_score(model2_ytrain, y_pred_train,pos_label = 1)))

print('\n')

print('Test accuracy : ' + str(accuracy_score(model2_ytest, y_pred)))
print('Test Precision : ' + str(precision_score(model2_ytest, y_pred,pos_label = 1)))
print('Test Recall : ' + str(recall_score(model2_ytest, y_pred,pos_label = 1)))
print('Test F1 score : ' + str(f1_score(model2_ytest, y_pred,pos_label = 1)))

print('\n')

########## Fit and test rfb SVM to the data. Perfrom a grid search to get the optimum hymerparameters

from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV

rsvm = SVC(kernel='rbf', random_state=0)

params = {'C': list(range(500, 1000, 25)),  
     'gamma': [0.02, 0.03, 0.05, 0.075, 0.1, 0.2, 0.35, 0.5, 0.7, 0.9 ] } 

grid_search_cv = GridSearchCV(rsvm, params, n_jobs = -1, cv = 5, scoring = 'f1')
grid_search_cv.fit(model2_Xtrain, model2_ytrain.values.ravel())
print(grid_search_cv.best_params_)

y_pred = grid_search_cv.predict(model2_Xtest)
y_pred_train = grid_search_cv.predict(model2_Xtrain)

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

print('\n')
print('Train accuracy : ' + str(accuracy_score(model2_ytrain, y_pred_train)))
print('Train Precision : ' + str(precision_score(model2_ytrain, y_pred_train,pos_label = 1)))
print('Train Recall : ' + str(recall_score(model2_ytrain, y_pred_train,pos_label = 1)))
print('Train F1 score : ' + str(f1_score(model2_ytrain, y_pred_train,pos_label = 1)))

print('\n')

print('Test accuracy : ' + str(accuracy_score(model2_ytest, y_pred)))
print('Test Precision : ' + str(precision_score(model2_ytest, y_pred,pos_label = 1)))
print('Test Recall : ' + str(recall_score(model2_ytest, y_pred,pos_label = 1)))
print('Test F1 score : ' + str(f1_score(model2_ytest, y_pred,pos_label = 1)))


print('\n')

# Train an Adaboost model

from sklearn.ensemble import AdaBoostClassifier

clf = AdaBoostClassifier(n_estimators=50, random_state=10)
params = {'n_estimators': list(range(20,40))}
grid_search_cv = GridSearchCV(clf, params, n_jobs = -1, cv = 5, scoring = 'f1')
grid_search_cv.fit(model2_Xtrain, model2_ytrain.values.ravel())
print(grid_search_cv.best_params_)

y_pred = grid_search_cv.predict(model2_Xtest)
y_pred_train = grid_search_cv.predict(model2_Xtrain)

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

print('\n')
print('Train accuracy : ' + str(accuracy_score(model2_ytrain, y_pred_train)))
print('Train Precision : ' + str(precision_score(model2_ytrain, y_pred_train,pos_label = 1)))
print('Train Recall : ' + str(recall_score(model2_ytrain, y_pred_train,pos_label = 1)))
print('Train F1 score : ' + str(f1_score(model2_ytrain, y_pred_train,pos_label = 1)))

print('\n')

print('Test accuracy : ' + str(accuracy_score(model2_ytest, y_pred)))
print('Test Precision : ' + str(precision_score(model2_ytest, y_pred,pos_label = 1)))
print('Test Recall : ' + str(recall_score(model2_ytest, y_pred,pos_label = 1)))
print('Test F1 score : ' + str(f1_score(model2_ytest, y_pred,pos_label = 1)))

print('\n')

# Train an XGBoost model
from xgboost import XGBClassifier
xgb = XGBClassifier(booster = 'gbtree', random_state=0)

params = {
        'min_child_weight': [1, 5, 10],
        'gamma': [0.5, 1, 1.5, 2, 5, 10],
        'subsample': [0.6, 0.8, 1.0],
        'colsample_bytree': [0.6, 0.8, 1.0],
        'max_depth': list(range(2,8))
        }

grid_search_cv = GridSearchCV(xgb, params, n_jobs = -1, cv = 5, scoring = 'f1')
grid_search_cv.fit(model2_Xtrain, model2_ytrain.values.ravel())
print(grid_search_cv.best_params_)

y_pred = grid_search_cv.predict(model2_Xtest)
y_pred_train = grid_search_cv.predict(model2_Xtrain)

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

print('\n')
print('Train accuracy : ' + str(accuracy_score(model2_ytrain, y_pred_train)))
print('Train Precision : ' + str(precision_score(model2_ytrain, y_pred_train,pos_label = 1)))
print('Train Recall : ' + str(recall_score(model2_ytrain, y_pred_train,pos_label = 1)))
print('Train F1 score : ' + str(f1_score(model2_ytrain, y_pred_train,pos_label = 1)))

print('\n')

print('Test accuracy : ' + str(accuracy_score(model2_ytest, y_pred)))
print('Test Precision : ' + str(precision_score(model2_ytest, y_pred,pos_label = 1)))
print('Test Recall : ' + str(recall_score(model2_ytest, y_pred,pos_label = 1)))
print('Test F1 score : ' + str(f1_score(model2_ytest, y_pred,pos_label = 1)))

print('\n')

"""## Plot the PR curves"""

X_train = X_train2
X_test = X_test2
y_train = y_train2
y_test = y_test2

"""### PR curve for test data"""

# PR curve was trained using the previously optimized hyperparameters

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_recall_curve
from matplotlib import pyplot

# Gaussuan NB model
from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
gnb.fit(X_train, y_train.values.ravel())
gnb_probs = gnb.predict_proba(X_test)[: , 1] # predict probabilities
gnb_precision, gnb_recall, _ = precision_recall_curve(y_test, gnb_probs)


## AdaBoost model
from sklearn.ensemble import AdaBoostClassifier
clf = AdaBoostClassifier(random_state=10,n_estimators = 23)
clf.fit(X_train, y_train.values.ravel())
clf_probs = clf.predict_proba(X_test)[: , 1] # predict probabilities
clf_precision, clf_recall, _ = precision_recall_curve(y_test, clf_probs)


## Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(criterion='gini',max_leaf_nodes = 8, n_estimators=50, random_state=0,n_jobs=2, bootstrap = True)
forest.fit(X_train, y_train.values.ravel())
forest_probs = forest.predict_proba(X_test)[: , 1] # predict probabilities
forest_precision, forest_recall, _ = precision_recall_curve(y_test, forest_probs)


## Linear SVM
from sklearn.svm import SVC
lsvm = SVC(kernel='linear', C = 90, random_state=0, probability=True)
lsvm.fit(X_train, y_train.values.ravel())
lsvm_probs = lsvm.predict_proba(X_test)[: , 1] # predict probabilities
lsvm_precision, lsvm_recall, _ = precision_recall_curve(y_test, lsvm_probs)

## KNN
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors = 15, metric = 'euclidean', p = 1)
knn.fit(X_train, y_train.values.ravel())
knn_probs = knn.predict_proba(X_test)[: , 1] # predict probabilities
knn_precision, knn_recall, _ = precision_recall_curve(y_test, knn_probs)


## Logistic Regression
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(random_state=0, solver='liblinear', C = 3)
lr.fit(X_train, y_train.values.ravel())
lr_probs = lr.predict_proba(X_test)[: , 1] # predict probabilities
lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs)

## Decision Tree
from sklearn import tree
tree = tree.DecisionTreeClassifier(random_state = 0, criterion = 'gini', max_depth = 2,min_samples_leaf = 1)
tree.fit(X_train, y_train.values.ravel())
tree_probs = tree.predict_proba(X_test)[: , 1] # predict probabilities
tree_precision, tree_recall, _ = precision_recall_curve(y_test, tree_probs)

## RBF SVM
from sklearn.svm import SVC
rsvm = SVC(kernel='rbf', random_state=0, C = 500, gamma = 0.05, probability = True)
rsvm.fit(X_train, y_train.values.ravel())
rsvm_probs = rsvm.predict_proba(X_test)[: , 1] # predict probabilities

## LDA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
lda = LDA()
lda.fit(X_train, y_train.values.ravel())
lda_probs = lda.predict_proba(X_test)[: , 1] # predict probabilities
lda_precision, lda_recall, _ = precision_recall_curve(y_test, lda_probs)

## XGBoost model
from xgboost import XGBClassifier
xgb = XGBClassifier(booster = 'gbtree', random_state=0, min_child_weight = 1, gamma = 0.5, subsample = 0.8,  colsample_bytree = 1, max_depth = 2 )
xgb.fit(X_train, y_train.values.ravel())
xgb_probs = xgb.predict_proba(X_test)[: , 1] # predict probabilities
xgb_precision, xgb_recall, _ = precision_recall_curve(y_test, xgb_probs)


# plot the precision-recall curves
from matplotlib.pyplot import figure
figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')
pyplot.plot(gnb_recall, gnb_precision, marker='.', label='Gaussian NB')
pyplot.plot(clf_recall, clf_precision, marker='.', label='AdaBoost')
pyplot.plot(forest_recall, forest_precision, marker='.', label='Random Forest')
pyplot.plot(lsvm_recall, lsvm_precision, marker='.', label='Linear SVM')
pyplot.plot(lda_recall, lda_precision, marker='.', label='LDA')
pyplot.plot(rsvm_recall, rsvm_precision, marker='.', label='RBF SVM')
pyplot.plot(knn_recall, knn_precision, marker='.', label='KNN')
pyplot.plot(tree_recall, tree_precision, marker='.', label='Decision Tree')
pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic Regression')
pyplot.plot(xgb_recall, xgb_precision, marker='.', label='XGBoost')
pyplot.xlabel('Recall')
pyplot.ylabel('Precision')
pyplot.legend(loc = 'upper right', bbox_to_anchor=(1.4, 0.7)) # show the legend
pyplot.show() # show the plot

# Save the PR data to plot offline

nan = np.empty(250)
nan[:] = np.nan

PR_all = pd.DataFrame(data = { 'GaussianNB Precision': np.append(gnb_precision, nan[0:250-len(gnb_precision)]) , 'GaussianNB Recall': np.append(gnb_recall, nan[0:250-len(gnb_precision)]) , 
                              'Random Forest Precision': np.append(forest_precision, nan[0:250-len(forest_precision)]) , 'Random Forest Recall': np.append(forest_recall, nan[0:250-len(forest_precision)]) , 
                              'AdaBoost Precision': np.append(clf_precision, nan[0:250-len(clf_precision)]) , 'AdaBoost Recall': np.append(clf_recall, nan[0:250-len(clf_precision)]),
                              'KNN Precision': np.append(knn_precision, nan[0:250-len(knn_precision)]) , 'LNN Recall': np.append(knn_recall, nan[0:250-len(knn_precision)]),
                              'LDA Precision': np.append(lda_precision, nan[0:250-len(lda_precision)]) , 'LDA Recall': np.append(lda_recall, nan[0:250-len(lda_precision)]),
                              'Decision Tree Precision': np.append(tree_precision, nan[0:250-len(tree_precision)]) , 'Decision Tree Recall': np.append(tree_recall, nan[0:250-len(tree_precision)]),
                              'Logistic Regression Precision': np.append(lr_precision, nan[0:250-len(lr_precision)]) , 'Logistic Regression Recall': np.append(lr_recall, nan[0:250-len(lr_precision)]),
                              'RBF SVM Precision': np.append(rsvm_precision, nan[0:250-len(rsvm_precision)]) , 'RBF SVM Recall': np.append(rsvm_recall, nan[0:250-len(rsvm_precision)]),
                              'Linear SVM Precision': np.append(lsvm_precision, nan[0:250-len(lsvm_precision)]) , 'Linear SVM Recall': np.append(lsvm_recall, nan[0:250-len(lsvm_precision)]),
                              'XGBoost Precision': np.append(xgb_precision, nan[0:250-len(xgb_precision)]) , 'XGBoost Recall': np.append(xgb_recall, nan[0:250-len(xgb_precision)])                                
                              })
PR_all.to_csv('PR_test_model2_endo.csv')
files.download('PR_test_model2_endo.csv')

"""### PR curve for combined train+test data"""

# PR curve was trained using the previously optimized hyperparameters

X_all = pd.concat([X_train, X_test])
y_all = pd.concat([y_train, y_test])

X_test = X_all
y_test = y_all

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_recall_curve
from matplotlib import pyplot

# Gaussuan NB model
from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
gnb.fit(X_train, y_train.values.ravel())
gnb_probs = gnb.predict_proba(X_test)[: , 1] # predict probabilities
gnb_precision, gnb_recall, _ = precision_recall_curve(y_test, gnb_probs)


## AdaBoost model
from sklearn.ensemble import AdaBoostClassifier
clf = AdaBoostClassifier(random_state=10,n_estimators = 23)
clf.fit(X_train, y_train.values.ravel())
clf_probs = clf.predict_proba(X_test)[: , 1] # predict probabilities
clf_precision, clf_recall, _ = precision_recall_curve(y_test, clf_probs)


## Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(criterion='gini',max_leaf_nodes = 8, n_estimators=50, random_state=0,n_jobs=2, bootstrap = True)
forest.fit(X_train, y_train.values.ravel())
forest_probs = forest.predict_proba(X_test)[: , 1] # predict probabilities
forest_precision, forest_recall, _ = precision_recall_curve(y_test, forest_probs)


## Linear SVM
from sklearn.svm import SVC
lsvm = SVC(kernel='linear', C = 90, random_state=0, probability=True)
lsvm.fit(X_train, y_train.values.ravel())
lsvm_probs = lsvm.predict_proba(X_test)[: , 1] # predict probabilities
lsvm_precision, lsvm_recall, _ = precision_recall_curve(y_test, lsvm_probs)

## KNN
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors = 15, metric = 'euclidean', p = 1)
knn.fit(X_train, y_train.values.ravel())
knn_probs = knn.predict_proba(X_test)[: , 1] # predict probabilities
knn_precision, knn_recall, _ = precision_recall_curve(y_test, knn_probs)


## Logistic Regression
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(random_state=0, solver='liblinear', C = 3)
lr.fit(X_train, y_train.values.ravel())
lr_probs = lr.predict_proba(X_test)[: , 1] # predict probabilities
lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs)

## Decision Tree
from sklearn import tree
tree = tree.DecisionTreeClassifier(random_state = 0, criterion = 'gini', max_depth = 2,min_samples_leaf = 1)
tree.fit(X_train, y_train.values.ravel())
tree_probs = tree.predict_proba(X_test)[: , 1] # predict probabilities
tree_precision, tree_recall, _ = precision_recall_curve(y_test, tree_probs)

## RBF SVM
from sklearn.svm import SVC
rsvm = SVC(kernel='rbf', random_state=0, C = 500, gamma = 0.05, probability = True)
rsvm.fit(X_train, y_train.values.ravel())
rsvm_probs = rsvm.predict_proba(X_test)[: , 1] # predict probabilities

## LDA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
lda = LDA()
lda.fit(X_train, y_train.values.ravel())
lda_probs = lda.predict_proba(X_test)[: , 1] # predict probabilities
lda_precision, lda_recall, _ = precision_recall_curve(y_test, lda_probs)

## XGBoost model
from xgboost import XGBClassifier
xgb = XGBClassifier(booster = 'gbtree', random_state=0, min_child_weight = 1, gamma = 0.5, subsample = 0.8,  colsample_bytree = 1, max_depth = 2 )
xgb.fit(X_train, y_train.values.ravel())
xgb_probs = xgb.predict_proba(X_test)[: , 1] # predict probabilities
xgb_precision, xgb_recall, _ = precision_recall_curve(y_test, xgb_probs)


# plot the precision-recall curves
from matplotlib.pyplot import figure
figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')
pyplot.plot(gnb_recall, gnb_precision, marker='.', label='Gaussian NB')
pyplot.plot(clf_recall, clf_precision, marker='.', label='AdaBoost')
pyplot.plot(forest_recall, forest_precision, marker='.', label='Random Forest')
pyplot.plot(lsvm_recall, lsvm_precision, marker='.', label='Linear SVM')
pyplot.plot(lda_recall, lda_precision, marker='.', label='LDA')
pyplot.plot(rsvm_recall, rsvm_precision, marker='.', label='RBF SVM')
pyplot.plot(knn_recall, knn_precision, marker='.', label='KNN')
pyplot.plot(tree_recall, tree_precision, marker='.', label='Decision Tree')
pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic Regression')
pyplot.plot(xgb_recall, xgb_precision, marker='.', label='XGBoost')
pyplot.xlabel('Recall')
pyplot.ylabel('Precision')
pyplot.legend(loc = 'upper right', bbox_to_anchor=(1.4, 0.7)) # show the legend
pyplot.show() # show the plot

# Save the PR data to plot offline

nan = np.empty(500)
nan[:] = np.nan

PR_all = pd.DataFrame(data = { 'GaussianNB Precision': np.append(gnb_precision, nan[0:500-len(gnb_precision)]) , 'GaussianNB Recall': np.append(gnb_recall, nan[0:500-len(gnb_precision)]) , 
                              'Random Forest Precision': np.append(forest_precision, nan[0:500-len(forest_precision)]) , 'Random Forest Recall': np.append(forest_recall, nan[0:500-len(forest_precision)]) , 
                              'AdaBoost Precision': np.append(clf_precision, nan[0:500-len(clf_precision)]) , 'AdaBoost Recall': np.append(clf_recall, nan[0:500-len(clf_precision)]),
                              'KNN Precision': np.append(knn_precision, nan[0:500-len(knn_precision)]) , 'LNN Recall': np.append(knn_recall, nan[0:500-len(knn_precision)]),
                              'LDA Precision': np.append(lda_precision, nan[0:500-len(lda_precision)]) , 'LDA Recall': np.append(lda_recall, nan[0:500-len(lda_precision)]),
                              'Decision Tree Precision': np.append(tree_precision, nan[0:500-len(tree_precision)]) , 'Decision Tree Recall': np.append(tree_recall, nan[0:500-len(tree_precision)]),
                              'Logistic Regression Precision': np.append(lr_precision, nan[0:500-len(lr_precision)]) , 'Logistic Regression Recall': np.append(lr_recall, nan[0:500-len(lr_precision)]),
                              'RBF SVM Precision': np.append(rsvm_precision, nan[0:500-len(rsvm_precision)]) , 'RBF SVM Recall': np.append(rsvm_recall, nan[0:500-len(rsvm_precision)]),
                              'Linear SVM Precision': np.append(lsvm_precision, nan[0:500-len(lsvm_precision)]) , 'Linear SVM Recall': np.append(lsvm_recall, nan[0:500-len(lsvm_precision)]),
                              'XGBoost Precision': np.append(xgb_precision, nan[0:500-len(xgb_precision)]) , 'XGBoost Recall': np.append(xgb_recall, nan[0:500-len(xgb_precision)])                                
                              })
PR_all.to_csv('PR_all_model2_endo.csv')
files.download('PR_all_model2_endo.csv')